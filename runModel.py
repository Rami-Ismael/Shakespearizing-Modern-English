# -*- coding: utf-8 -*-
"""Working  Getting started with TensorFlow_Keras for Machine Translation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QarVk0HIJfLNKmVZH0SJmaLypBNWBMrQ
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# import dependencies
import os
import tensorflow as tf
import datetime
from tensorflow.keras.callbacks import TensorBoard
import time
import Constant
import manipulateFolder 
import zipfile
from tensorflow import keras
from tensorflow.keras.callbacks import Callback, History, CSVLogger
import numpy as np
import kerasEarlyStop
import DeepLearningModel
from blueScore import BLEU
NAME =""
type_of_model=""

"""# Data"""


modern = 'data/train.modern.nltktok'
original = 'data/train.original.nltktok'

"""# Extract vocabulary"""

"""## Modern English"""


# read text
with open(modern) as file:
  text = file.read()

# tokenize
text = text.lower()
text = text.split('\n')
tokens = []
for sent in text:
  tokens.extend(sent.split(' '))

# get vocabs
source_vocabs = ['[UNK]', '[PAD]'] + list(set(tokens))

print("Vocabulary size = {}".format(len(source_vocabs)))

"""## Shakespearse's English"""

# read text
with open(original) as file:
  text = file.read()

# tokenize
text = text.lower()
text = text.split('\n')
tokens = []
for sent in text:
  tokens.extend(sent.split(' '))

# get vocabs
target_vocabs = ['[UNK]', '[PAD]'] + list(set(tokens))

print("Vocabulary size = {}".format(len(target_vocabs)))

target_vocabs[0]

"""## Load data"""

def load_data(modern, original, source_vocabs, target_vocabs):
    #  read data
    modern = tf.data.TextLineDataset(modern)
    original = tf.data.TextLineDataset(original)

    # batching
    batch_size = Constant.BATCH_SIZE # multiple of 2: 2, 4, 8, 16, 32, 64
    modern = modern.batch(batch_size)
    original = original.batch(batch_size)

    ## lower case
    lower_case = lambda x: tf.strings.lower(x)
    modern = modern.map(lower_case)
    original = original.map(lower_case)

    ## tokenize
    split = lambda x: tf.strings.split(x)
    modern = modern.map(split)
    original = original.map(split)

    ## remove stopwords/punctuations
    ## please explore the list of stopword and punctuations and remove them
    ## use tf.strings.regex_replace to remove all stopword and puncutations
    ## your codes go here

    ## padding & truncation 
    def pad(text):
        """
        text : RaggedTensor (list of variable-length elements)
        hence, use tf.map_fn
        """
        max_length = 50
        def _pad(input):
            if tf.size(input) < max_length:
                paddings = tf.repeat(tf.constant('[PAD]', dtype = tf.string), max_length - tf.size(input))
                return tf.concat([input, paddings], axis = 0)
            else:
                return input[:max_length]
            
        # map _pad for each string
        text = tf.map_fn(_pad, text)

        return text.to_tensor() # text is tf.RaggedTensor for dynamic length -> converted to fixed length
    # padding
    modern = modern.map(pad)
    original = original.map(pad)

    # encode into integers
    modern_table = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            source_vocabs, tf.constant(list(range(len(source_vocabs))))
        ), default_value = 0
    )
    
    original_table = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            target_vocabs, tf.constant(list(range(len(target_vocabs))))
        ), default_value = 0
    )
    
    modern = modern.map(lambda text: modern_table.lookup(text))
    original = original.map(lambda text: original_table.lookup(text))

    dataset = tf.data.Dataset.zip((modern, original))
    return dataset

train_dataset = load_data(modern, original, source_vocabs, target_vocabs)

for sample in train_dataset:
  print('modern', sample[0])
  print('original', sample[1].shape)
  break

val_modern = 'data/valid.modern.nltktok'
val_original = 'data/valid.original.nltktok'

val_dataset = load_data(val_modern, val_original, source_vocabs, target_vocabs)


"""# Model and Tensorboard"""
#create a deep learning model 
DeepModel = DeepLearningModel.Forward_LSTM(len(source_vocabs),len(target_vocabs),sequence_length=Constant.SEQUENCE_LENGTH)
model = DeepModel[0]
type_of_model = DeepModel[1]
print(model.summary())

"""Naming the Model"""


def name_model(epochs,type_of_model,learning_rate,loss_function,details):
    return f'{type_of_model}_E_{epochs}_LR{learning_rate}_LF{loss_function}_BACH_S{Constant.BATCH_SIZE}_SEQL{Constant.SEQUENCE_LENGTH} '+details
#Early stop in keras


"""# Hyperparameters"""

learning_rate = Constant.LEARNING_RATE # 0.0001, 0.00xxx1
##Learning rate scheule 
lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  Constant.LEARNING_RATE,
  decay_steps=Constant.BATCH_SIZE*1000,
  decay_rate=Constant.DECAY_RATE,
  staircase=False)

optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)
metrics = [BLEU()] # BLEU
model.compile(optimizer = optimizer, loss = loss, metrics = metrics)
"""Nameing the tensorboad
"""
print(type_of_model)
loss_function = Constant.SCC
NAME = name_model(Constant.EPOCHS,type_of_model,learning_rate,loss_function,"")
NAME = NAME+' '+format(datetime.datetime.now())
NAME = NAME.replace(":","_")
csv_Name = NAME+".log"
"""# Tranining

"""
history = History()
logs = Callback()
csv_logger = CSVLogger(csv_Name)
tensorboard  = TensorBoard(log_dir="logs/{}".format(NAME))
epochs = Constant.EPOCHS
model.fit(train_dataset, validation_data = val_dataset, epochs = epochs, verbose = 1, callbacks=[history,csv_logger,tensorboard,kerasEarlyStop.EarlyStoppingAtMinLossWithPatience()])


"""## Save model"""

size = len(history.history['loss'])
model_loss = history.history['loss'][size-1]
model_loss_formated = format(model_loss,".5f")
model_validation_loss = history.history['val_loss'][size-1]
model_validation_loss_formated = format(model_validation_loss,".5")
name_of_model = NAME
location_of_folder = "/model/"
model.save(location_of_folder+name_of_model)

#move the csv file to the

manipulateFolder.moveFileIntoDir(csv_Name,"csv")


'''model = tf.keras.load_model(path)
model.predict(input)'''
print(name_of_model)
print(csv_Name)
print(type_of_model)
print(history.history)
print(history.history.keys)
print("The model lost "+str(model_loss)+ "The validation lost "+str(model_validation_loss))

