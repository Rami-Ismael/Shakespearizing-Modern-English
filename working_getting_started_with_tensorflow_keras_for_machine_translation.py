# -*- coding: utf-8 -*-
"""Working  Getting started with TensorFlow_Keras for Machine Translation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QarVk0HIJfLNKmVZH0SJmaLypBNWBMrQ
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# import dependencies
import os
import tensorflow as tf
import datetime
from tensorflow.keras.callbacks import TensorBoard
import time
NAME =""

"""# Data"""


modern = 'data/train.modern.nltktok'
original = 'data/train.original.nltktok'

"""# Extract vocabulary"""

"""## Modern English"""


# read text
with open(modern) as file:
  text = file.read()

# tokenize
text = text.lower()
text = text.split('\n')
tokens = []
for sent in text:
  tokens.extend(sent.split(' '))

# get vocabs
source_vocabs = ['[UNK]', '[PAD]'] + list(set(tokens))

print("Vocabulary size = {}".format(len(source_vocabs)))

"""## Shakespearse's English"""

# read text
with open(original) as file:
  text = file.read()

# tokenize
text = text.lower()
text = text.split('\n')
tokens = []
for sent in text:
  tokens.extend(sent.split(' '))

# get vocabs
target_vocabs = ['[UNK]', '[PAD]'] + list(set(tokens))

print("Vocabulary size = {}".format(len(target_vocabs)))

target_vocabs[0]

"""## Load data"""

def load_data(modern, original, source_vocabs, target_vocabs):
    #  read data
    modern = tf.data.TextLineDataset(modern)
    original = tf.data.TextLineDataset(original)

    # batching
    batch_size = 256 # multiple of 2: 2, 4, 8, 16, 32, 64
    modern = modern.batch(batch_size)
    original = original.batch(batch_size)

    ## lower case
    lower_case = lambda x: tf.strings.lower(x)
    modern = modern.map(lower_case)
    original = original.map(lower_case)

    ## tokenize
    split = lambda x: tf.strings.split(x)
    modern = modern.map(split)
    original = original.map(split)

    ## remove stopwords/punctuations
    ## please explore the list of stopword and punctuations and remove them
    ## use tf.strings.regex_replace to remove all stopword and puncutations
    ## your codes go here

    ## padding & truncation 
    def pad(text):
        """
        text : RaggedTensor (list of variable-length elements)
        hence, use tf.map_fn
        """
        max_length = 50
        def _pad(input):
            if tf.size(input) < max_length:
                paddings = tf.repeat(tf.constant('[PAD]', dtype = tf.string), max_length - tf.size(input))
                return tf.concat([input, paddings], axis = 0)
            else:
                return input[:max_length]
            
        # map _pad for each string
        text = tf.map_fn(_pad, text)

        return text.to_tensor() # text is tf.RaggedTensor for dynamic length -> converted to fixed length
    # padding
    modern = modern.map(pad)
    original = original.map(pad)

    # encode into integers
    modern_table = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            source_vocabs, tf.constant(list(range(len(source_vocabs))))
        ), default_value = 0
    )
    
    original_table = tf.lookup.StaticHashTable(
        tf.lookup.KeyValueTensorInitializer(
            target_vocabs, tf.constant(list(range(len(target_vocabs))))
        ), default_value = 0
    )
    
    modern = modern.map(lambda text: modern_table.lookup(text))
    original = original.map(lambda text: original_table.lookup(text))

    dataset = tf.data.Dataset.zip((modern, original))
    return dataset

train_dataset = load_data(modern, original, source_vocabs, target_vocabs)

for sample in train_dataset:
  print('modern', sample[0])
  print('original', sample[1].shape)
  break

val_modern = 'data/valid.modern.nltktok'
val_original = 'data/valid.original.nltktok'

val_dataset = load_data(val_modern, val_original, source_vocabs, target_vocabs)

for sample in val_dataset:
  print('modern', sample[0])
  print('original', sample[1].shape)
  break

"""# Model and Tensorboard"""

def model_fn(source_vocab_size, target_vocab_size, sequence_length):
  # input
  input = tf.keras.Input(shape = (sequence_length))

  # embedding layer
  embedding = tf.keras.layers.Embedding(source_vocab_size, 300) # vocabs = 10000, embed_dim = 64, sequence_length = 10
  embedding = embedding(input)

  # LSTM encoder
  lstm = tf.keras.layers.LSTM(256, return_sequences = True, name = 'Encoder_1',activation="tanh", recurrent_activation="sigmoid", recurrent_dropout=0.0,unroll=False,use_bias=True)(embedding)
  lstm = tf.keras.layers.LSTM(512, return_sequences = True, name = 'Encoder_2',activation="tanh", recurrent_activation="sigmoid", recurrent_dropout=0.0,unroll=False,use_bias=True)(lstm)

  # LSTM decoder
  lstm = tf.keras.layers.LSTM(128, return_sequences = True, name = 'Decoder',activation="tanh", recurrent_activation="sigmoid", recurrent_dropout=0.0,unroll=False,use_bias=True)(lstm)

  # output
  output = tf.keras.layers.Dense(target_vocab_size)(lstm)

  return tf.keras.Model(inputs = [input], outputs = [output])

model = model_fn(len(source_vocabs), len(target_vocabs), sequence_length = 50)
print(model.summary())

"""Naming the Model"""

def name_model(epochs,type_of_model,learning_rate,loss_function, loss_value, val_value):
    return f' model is {type_of_model} the epochs {epochs}  learning_rate {learning_rate} ' \
           f'lost function is {loss_function}  model loss is {loss_value}  model validation loss  {val_value}'

"""# Hyperparameters"""

learning_rate = 0.01 # 0.0001, 0.00xxx1
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)
metrics = [] # BLEU
model.compile(optimizer = optimizer, loss = loss, metrics = metrics)

"""# Tranining

"""
from tensorflow import keras
from tensorflow.keras.callbacks import Callback, History, CSVLogger
history = History()
logs = Callback()
csv_logger = CSVLogger('training.log')
tensorboard  = TensorBoard(log_dir="logs/{}".format(NAME))
epochs = 1
model.fit(train_dataset, validation_data = val_dataset, epochs = epochs, verbose = 1, callbacks=[history,csv_logger,tensorboard])


"""## Save model"""

type_of_model = "seq_seq"
loss_function = "Sparse_Categorigical_Crossentropy"
size = len(history.history['loss'])
model_loss = history.history['loss'][size-1]
model_loss_formated = format(model_loss,".5f")
model_validation_loss = history.history['val_loss'][size-1]
model_validation_loss_formated = format(model_validation_loss,".5")
name_of_model = name_model(epochs,type_of_model,learning_rate,loss_function,model_loss_formated,model_validation_loss_formated)
name_of_model_zip = name_of_model+".zip"
NAME = NAME.format(datetime.datetime.now())
location_of_folder = "/model/"
model.save(location_of_folder+name_of_model)
## move the training log csv into the model save directory

import os
import zipfile
from tensorflow.python.keras.callbacks import CSVLogger
def zipdir(path, ziph):
    # ziph is zipfile handle
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file))
zipf = zipfile.ZipFile(name_of_model_zip, 'w', zipfile.ZIP_DEFLATED)
zipdir("/content/"+name_of_model, zipf)
zipf.close()



'''model = tf.keras.load_model(path)
model.predict(input)'''

"""# History"""
print(history)
print(type(history))
dir(history.history.keys())
print(history.history.keys())
print(history.history['loss'])
print(history.history['loss'][size-1])
